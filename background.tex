\chapter{Background}
\label{chap:background}

This chapter outlines the preliminary information necessary to contextualize the
remainder of this thesis for readers unfamiliar with existing CRDT research.
Here we motivate conflict-free replicated datatypes (CRDTs), formalize state-
and op-based variants of CRDTs, and present examples of common instantiations.
Finally, we conclude with a discussion of the different levels of consistency
guarantees that each CRDT variant offers, and rationalize which levels of
consistency are appealing in certain situations.

\section{Coordinated Replication}
In a distributed system, it is common for more than one participant to need to
have a \textit{view} of the same data. For example, multiple nodes may need to
have access to the same internal data structures necessary to execute some
computation. When a piece of data is shared among many participants in a system,
we say that that data is \textit{replicated}.

However, saying only that some data is ``replicated'' is under-specified. For
example: how often is that data updated among multiple participants? How does
that data behave when multiple participants are modifying it concurrently? Do
all participants always have the same view of the data, or are there temporary
divergences between the participants in the system?

It turns out that the answer of the last question is of paramount importance.
Traditionally speaking, in a distributed system, all participants have an
identical replica of any piece of shared data at all times. That is, at no
moment in time will there be a replica that could atomically compare its
replicated value for some data with any other replica for equality and disagree.
Said otherwise, all replicated values are equal everywhere all at once. This is
an appealing property to say the least, because it allows system designers to
conceptually treat a distributed system as a single unit of computation. That
is, if all replicas maintain the same memory, it is conceptually as if one whole
machine is being replicated many times.

That being said, upholding this requirement is not a straightforward task. An
immediate question arises which is: who coordinates when updates to a piece of
data are replicated to other participants in the system? What happens when the
coordinator becomes unresponsive, or otherwise misbehaves? Who is responsible
for electing a new participant to take over the coordination duties of the
participant which was no longer able to fulfill them?

\section{Distributed Consensus Algorithms}

This gives rise to the area of consensus algorithms. A consensus algorithm,
broadly speaking, is a routine which multiple participants follow in order to
agree on a shared value.

We first state briefly the properties that an algorithm must have to solve
distributed consensus:
\begin{definition}[Distributed Consensus Algorithm, \citep{howard19}]
  \label{def:consensus}
  An algorithm is said to solve distributed consensus if it has the following
  three safety requirements:
  \begin{description}
    \item[Non-triviality] The decided value must have been proposed by a
      participant.
    \item[Safety] Once a value has been decided, no other value will be decided.
    \item[Safe learning] If a participant learns a value, it must learn the
      decided value.
  \end{description}
  In addition, it must satisfy the following two progress requirements:
  \begin{description}
    \item[Progress] Under previously agreed-upon liveness conditions, if a value
      is proposed by a participant, then a value is eventually decided.
    \item[Eventual learning] Under the same conditions as above, if a value is
      decided, then that value must be eventually learned.
  \end{description}
\end{definition}

We summarize the two most popular and often-implemented algorithms for
distributed consensus~\citep{howard20}:

\paragraph{Paxos}
The most popular algorithm in this field is Paxos~\citep{lamport98}. Broadly
speaking, values (corresponding to changing the value of a replicated piece of
data) are \textit{proposed}, \textit{accepted}, and \textit{learned} by
participants in the system. This process is coordinated by an elected
\textit{leader}, which is responsible for communicating with other participants
in order to drive the process forward. When a leader becomes unresponsive, other
participants in the system begin an election process to replace the leader, and
do so when a majority of participants (and the new leader) acknowledge the
  change.

\paragraph{Raft}
Lest we omit another often-implemented consensus algorithm, we briefly discuss
the Raft consensus algorithm as well~\citep{ongaro14}. Raft emphasizes
understandability in its design, and ``separates the key elements of consensus''
by silo-ing replication, leader election, and safety into different sub-problems
at the design level. An execution of Raft consists of several \textit{terms}. To
begin each term, an election is held in order to determine a \textit{leader}.
Once elected, the leader is responsible for disseminating updates to each
replicas copy of the \textit{log}. Conflicting log entries are always resolved
in favor of the leader's copy. Finally, Raft enforces safety by imposing
additional restrictions on the behavior of a term such that the log replication
strategy is proven to be safe. This is argued in~\citep{ongaro14} and
mechanically verified in~\citep{wilcox15}.

\subsection{Safety in Distributed Consensus Algorithms}

Both Raft and Paxos are notoriously difficult to implement correctly in
practice~\citep{howard20}. Distributed consensus algorithms are often the
subject of undergraduate-level courses in networks and distributed systems.
Often, commercially-available implementations of these algorithms are built into
off the shelf solutions, such as Apache Zookeeper and CoreOS's

It is a natural question to ask why these algorithms are so notoriously
difficult to implement in practice. Individually, the properties
in Definition~\ref{def:consensus} seem reasonably in their own right. We propose
that it is the safety property (that once a value has been decided, no other
values will be decided) that makes implementing these algorithms correctly so
difficult in practice. In essence, coordinating the proposals individual
participants submit is the central difficulty of these algorithms.

Conflict-free Replicated Datatypes (CRDTs) are a natural answer to this
question. By allowing participants to temporarily diverge from the state of the
overall computation (c.f., the second property of
Definition~\ref{def:eventual-consistency}), CRDTs allow replicas to violate the
safety property of Definition~\ref{def:consensus}. By giving up the immediacy
and permanence that the safety properties of a traditional distributed consensus
algorithm, CRDTs allow for a dramatically lower implementation burden in
practice, and are substantially easier to reason about.

\section{state-based CRDTs}
We begin with a discussion of state-based CRDTs from their inception
in~\citep{shapario11}. A state-based CRDT is a 5-tuple $(S, s^0, q, u, m)$. An
individual replica of a state-based CRDT is at some state $s^i \in S$ for $i \ge
0$, and is initially $s^0$. The value may be queried by any client or other
replica by invoking $q$. It may be updated with $u$, which has a unique type per
CRDT object. Finally, $m$ merges the state of some other remote replica.

Crucially, the states of a given state-based CRDT form a partially-ordered set
$\langle S, \sqsubseteq \rangle$. This poset is used to form a join
semi-lattice, where any finite subset of elements has a natural least
upper-bound. Consider two elements $s^m, s^n \in S$. The least upper-bound
$s = s^m \sqcup s^n$ is given as:
\[
  \forall s'.\; s' \sqsupseteq s^m, s^n \Rightarrow
    s^m \sqsubseteq s \land
    s^n \sqsubseteq s \land
    s \sqsubseteq s'
\]
In other words, a $s = s^m \sqcup s^n$ is a least upper-bound of $s^m$ and $s^n$
if it is the smallest element that is at least as large as both $s^m$ and $s^n$.

For now, we set aside the query operations $q$, and allow $q$'s implementation
detail that is left to individual CRDTs, and is specific to their use-case. For
e.g., it may be that a vector-counter CRDT's implementation of $q$ is
something like:
\[
  q : S \mapsto \mathbb{N} = \sum_{i \in \mathbb{I}} s(i)
\]
In this example, let $S = \mathbb{N}^{|\mathbb{I}|}_+$, where $\mathbb{I}$ is
the set of replicas, and $s(i)$ returns the $i$th index of the state vector.

We'll focus our attention for now on the implementation of $m$, which is the
function responsible for merging the states of two independent replicas of a
given state-based CRDT. Given a suitable set of states which forms a lattice, we
assume that:
\[
  m(s_1, s_2) = s_1 \sqcup s_2
\]
and that whenever a CRDT replica $r_1$ at state $s_1$ receives an update from
another replica $r_2$ at state $s_2$, that $r_1$ attains a new state $s_1' =
m(s_1, s_2)$. This process, in addition to each replica periodically
broadcasting an update which contains its current state, is carried on
continually, and $m$ is invoked whenever a new state is received. That is, each
replica is evolving over time in response to outside instruction, and in turn
these updates cause internal state transitions, which themselves cause those new
states to be broadcasted and eventually joined at every other replica.

The $\sqcup$ operator has three mathematical properties that make it an
appealing choice for joining states together as in $m$. These are its
\emph{commutativity}, \emph{associativity}, and \emph{idempotency}. That is, for
any states $s_1$, $s_2$, and $s_3$, that:
\begin{itemize}
  \item The operator is \emph{commutative}, i.e., that $s_1 \sqcup s_2 = s_2
    \sqcup s_1$.
  \item The operator is \emph{associative}, i.e., that $s_1 \sqcup (s_2 \sqcup
    s_3) = (s_1 \sqcup s_2) \sqcup s_3$.
  \item Finally, the operator is \emph{idempotent}, i.e., that $s_1 \sqcup s_1 =
    s_1$.
\end{itemize}

These mathematical properties correspond to real-world constraints that often
arise naturally in the area of distributed systems. Take, for example, that
messages may occur out of order. This often happens in, for example, UDP (User
Datagram Protocol) networks, where the received datagrams are not guaranteed to
be in the order that they were sent. Because $\sqcup$ is commutative, replicas
joining the updates of other replicas do not need to receive those updates in
order, because the result of $s_1 \sqcup s_2$ is the same as $s_2 \sqcup s_1$.
That is, it does not matter which of two updates from another replica arrives
first, because the result is the same no matter in which order they are
delivered.

For concreteness, say that we have two replicas, $r_1$ and $r_2$. $r_1$
initially begins at state $s$, and $r_2$ progresses through states $s_1, \ldots,
s_n$ for $n > 0$. We then see that it does not matter the order in which these
updates are delivered to $r_1$. Suppose that we have a mapping $\pi : [n] \to
[n]$ which maps the true order of a state $s_i$ to the order in which it was
delivered. Then, we can see that the choice of $\pi$ is arbitrary, because:
\[
  s \gets s \sqcup (s_{\pi(1)} \sqcup \cdots \sqcup s_{\pi(n)})
\]
for any choice of $\pi$, because
\[
  s_{\pi(1)} \sqcup \cdots \sqcup s_{\pi(n)} = s_1 \sqcup \cdots \cdots s_n
\]
which follows from the fact that $\sqcup$ is commutative, and can be shown
inductively on the number of updates, $n$.

Next, it is often common for packets to be duplicated in transit over a network.
That is, even though a packet may be sent from a source only once, it may be
received by a recipient on the same network multiple times. For this, the
idempotency of $\sqcup$ comes in handy: no matter how many times a state is
broadcast from an evolving replica, any other replica on the network will
tolerate that set of messages, because it only requires the message to be
delivered once. Any additional duplicates are merged in without changing the
state.

Finally, associativity is an appealing property, too, although its applications
are both less immediate and less often-used in this thesis. Suppose that several
replicas of a state-based CRDT reside on a network with, say, high latency, or
it is otherwise undesirable to send more messages on the network than is
necessary. Because associativity implies that the grouping of updates is
arbitrary, a replica can maintain a \textit{set} of pending updates, and
periodically\footnote{``Periodically'' is arbitrary and an implementation
choice, but it would be easy to imagine that this could be interpreted as
whenever the set reaches a certain size, and/or after a certain amount of time
has passed since flushing the set of pending updates.} send that set to other
replicas by first folding $\sqcup$ over it and sending a single update.

Although commutativity and idempotency receive the greatest share of attention
within CRDT research, associativity is most often used in the area of
$\delta$-state CRDTs. We will discuss a couple of applications of the
associative property of $\sqcup$ in detail in Chapter~\ref{chap:future-work},
and overview them briefly now. The associativity of updates means that
$\delta$-state CRDTs means that these objects can form an \emph{delta interval},
say, $\Delta^a_b$, which comprises the updates on the interval of time $[a,b)$.
When certain restrictions on when an interval $\Delta^a_b$ may be flushed based
on the values of $a$, $b$, other properties may be attained such as causal
consistency~\citep{almedia18}. In this thesis, we do not investigate this
particular application further, although it appears to be a promising future
direction.

\section{op-based CRDTs}
\label{sec:op-based-crdts}

\section{Conflict Free Replicated Datatypes}
\subsection{Example: Grow-Only Counter}
\label{sec:example-gcounter}

\subsection{Example: PN-Counter}
\subsection{Example: OR-Set}
\section{Consistency Guarantees}
\subsection{Causal Consistency}
\subsection{Eventual Consistency}
\subsection{Strong Eventual Consistency}
\section{Network Semantics}
