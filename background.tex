\chapter{Background}
\label{chap:background}

This chapter outlines the preliminary information necessary to contextualize the
remainder of this thesis for readers unfamiliar with existing \CRDT research.
Here we motivate conflict-free replicated datatypes (\CRDTs), formalize state-
and op-based variants of \CRDTs, and present examples of common instantiations.
Finally, we conclude with a discussion of the different levels of consistency
guarantees that each \CRDT variant offers, and rationalize which levels of
consistency are appealing in certain situations.

\section{Motivation}
Conflict-free Replicated Datatypes (\CRDTs) are a way to store several copies of
a data-structure on multiple computers which form a distributed system. Each
participant in the system can make modifications to the datatype without
the need for explicit coordination with other participants. \CRDT
implementations are designed so that coordination-free updates which may
conflict with one another always have a deterministic resolution. This allows
multiple participants to query and modify their \emph{view} of the replicated
datatype, without the traditional overhead and implementation burden that more
stringent replication algorithms require.

Here, we'll discuss three variants of \CRDTs: state-based, op-based , and
$\delta$-state based. Each of these variants achieve a consistent value by the
use of different message types, and each likewise requires a different set of
delivery semantics. In this chapter, we identify $\delta$-state \CRDTs as
achieving an appealing set of trade-offs among each of the four variants. We
restate that they are able to achieve Strong Eventual Consistency (\SEC, the
best reasonably-achievable consistency guarantee for most \CRDT applications)
while maintaining both:
\begin{itemize}
  \item A relatively small payload size, as is the benefit of op-based \CRDTs,
    and
  \item Relatively weak delivery semantics, as is the benefit of state-based
    \CRDTs.
\end{itemize}

\section{Coordinated Replication}
In a distributed system, it is common for more than one participant to need to
have a \textit{view} of the same data. For example, multiple nodes may need to
have access to the same internal data structures necessary to execute some
computation. When a piece of data is shared among many participants in a system,
we say that that data is \textit{replicated}.

However, saying only that some data is ``replicated'' is under-specified. For
example: how often is that data updated among multiple participants? How does
that data behave when multiple participants are modifying it concurrently? Do
all participants always have the same view of the data, or are there temporary
divergences among the participants in the system?

It turns out that the answer to the last question is of paramount importance.
Traditionally speaking, in a distributed system, all participants have an
identical replica of any piece of shared data at all times. That is, at no
moment in time will there be a replica that could atomically compare its
replicated value for some data with any other replica for equality and disagree.
Said otherwise, all replicated values are equal everywhere all at once. This is
an appealing property to say the least, because it allows system designers to
conceptually treat a distributed system as a single unit of computation. That
is, if all replicas maintain the same memory, it is conceptually as if one whole
machine is being replicated many times.

That being said, upholding this requirement is not a straightforward task. An
immediate question arises which is: who coordinates when updates to a piece of
data are replicated to other participants in the system? What happens when the
coordinator becomes unresponsive, or otherwise misbehaves? Who is responsible
for electing a new participant to take over the coordination duties of the
participant which was no longer able to fulfill them?

\section{Distributed Consensus Algorithms}

This gives rise to the area of consensus algorithms. A consensus algorithm,
broadly speaking, is a routine which multiple participants follow in order to
agree on a shared value.

We first state briefly the properties that an algorithm must have to solve
distributed consensus:
\begin{definition}[Distributed Consensus Algorithm, \citep{howard19}]
  \label{def:consensus}
  An algorithm is said to solve distributed consensus if it has the following
  three safety requirements:
  \begin{enumerate}
    \item \emph{Non-triviality}: The decided value must have been proposed by a
      participant.
    \item \emph{Safety}: Once a value has been decided, no other value will be
      decided.
    \item \emph{Safe learning}: If a participant learns a value, it must learn
      the decided value.
  \end{enumerate}
  In addition, it must satisfy the following two progress requirements:
  \begin{enumerate}
    \item \emph{Progress}: Under previously agreed-upon liveness conditions, if
      a value is proposed by a participant, then a value is eventually decided.
    \item \emph{Eventual learning}: Under the same conditions as above, if a
      value is decided, then that value must be eventually learned.
  \end{enumerate}
\end{definition}

We summarize the two most popular and often-implemented algorithms for
distributed consensus~\citep{howard20}:

The two most popular algorithms in this field are Paxos and
Raft~\citep{howard20,lamport98,ongaro14}. Both algorithms implement a
distributed state-machine replication and can be used to implement linearizable
systems. Both of these systems are notoriously difficult to implement correctly
in practice~\citep{howard20}. The topics often appear in undergraduate-level
courses in Distributed Systems, and have been the subject of extensive
verification effort to date~\citep{wilcox15}. Often, these distributed systems
verification efforts require an enormous amount of effort.  In a companion
paper~\citet{woos16} uses 45,000 lines to verify the complete Raft protocol in
their system.

It is natural to ask what is the property of these systems that makes them
difficult to implement or reason about correctly in practice. One possible
answer is to look at the stringent safety requirements (that is, that once a
value has been decided, no other value(s) will be decided) in these algorithms.

\CRDTs are a natural response to this. By allowing participants to temporarily
diverge from the state of the overall computation (c.f., the second property of
Definition~\ref{def:eventual-consistency}), \CRDTs allow replicas to violate the
safety property of Definition~\ref{def:consensus}.  By giving up the immediacy
and permanence that the safety properties of a traditional distributed consensus
algorithm, \CRDTs allow for a dramatically lower implementation burden in
practice, and are substantially easier to reason about.

\section{Consistency Guarantees}
\CRDTs are said to attain a weaker form of consistency known as \emph{strong
eventual consistency} (hereafter, \SEC)~\citep{shapiro11}. \SEC is a refinement
of \emph{eventual consistency} (\EC). Informally, \EC says that reads from a
system eventually return the same value at all replicas, while \SEC says that if
any two nodes have received the same set of updates, they will be in the same
state.

\EC and the \SEC extension are natural answers to the question we pose in
Section~\ref{sec:dca-safety}. That is, we posit that it is the safety
requirement in traditional Distributed Consensus Algorithms which make them
difficult to implement correctly. \EC makes only a liveness guarantee, and so on
its own it is not a sufficient solution for handling distributed consensus in an
environment with relaxed requirements. \SEC, however, does add a safety
guarantee, but the precondition (namely that only nodes which have received the
same \emph{set} of updates will be in the same state) makes it possible to relax
our requirements around network delays, or particulars of a \CRDT algorithm
which do not send updates to all other replicas immediately.

In short, we believe that it is this relaxation--that is, that \CRDTs are only
required to be in the same state \emph{eventually}, conditioned on which updates
they have and have not yet received--which makes \SEC an appealing consistency
property for distributed systems which more relaxed requirements than would be
satisfied by a linearizable system.

We discuss each of these consistency classes in turn.

\subsection{Eventual Consistency}
\EC captures the informal guarantee that if all clients stop submitting updates
to the system, all replicas in the system eventually reach the same
value~\citep{shapiro11}. More formally, \EC requires the following three
properties~\citep{shapiro11}:
\begin{enumerate}
  \item \emph{Eventual delivery}. An update delivered at some correct replica is
    eventually delivered at all replicas.
    \[
      \forall r_1, r_2.\, f \in (\textsf{delivered}~r_1) \Rightarrow \Diamond f
      \in (\textsf{delivered}~r_2)
    \]
  \item \emph{Convergence}. Correct replicas which have received the same
    \emph{set} of updates eventually reflect the same state.
    \[
      \forall r_1, r_2.\,~\square~(\textsf{delivered}~r_1) =
      (\textsf{delivered}~r_2) \Rightarrow \Diamond~\square~q(r_1) = q(r_2)
    \]
  \item \emph{Termination}. All method executions terminate.
\end{enumerate}

\EC is a relatively weak form of consistency. In~\citet{shapiro11}, it is
observed that \EC systems will execute an update immediately only to discover
that it produces a conflict with some future update, and so a roll-back is
performed. This imposes an additional constraint, which is that replicas need
to form consensus on the ``standard'' way to resolve conflicts so that the same
conflicts are resolved identically at different replicas.

We devote some additional discussion to the first property of \EC. Eventual
delivery requires that all updates delivered to some correct replica are
eventually delivered to all other correct replicas. This property alone permits
too much of the underlying network, and so it can make it difficult to reason
about strong consistency guarantees over an unreliable network.

Take for an example a network which never delivers any messages. In this case,
the precondition for eventual delivery is not met, and so we are relieved of the
obligation to prove that updates are propagated to other replicas, since they
aren't delivered anywhere in the first place. However, consider a network which
delivers only the \emph{first} message sent on it, and then drops all other
messages. In this case, it \emph{is} possible that a replica will receive some
update, attempt to propagate it to other replicas, only for them to never be
delivered.

To resolve this conflict in practice, one of two approaches if often taken. In
the first approach, we assume a fair-loss network~\citep{cachin11} in which each
message has a non-zero probability of being delivered. To ensure that messages
are delivered, each node sends each message an infinite number of times over the
network, such that it will be delivered an infinite number of times. This
resolves the eventual delivery problem since we assumed a sufficient (but
weaker) condition of the underlying network, and then showed that we can
implement eventual delivery on top of these network semantics.

In the second approach, we first consider a set of delivery semantics $P$ which
predicates allowed and disallowed network behaviors. Typically, $P$ is assumed
to preserve causal order.\footnote{This is a standard
assumption~\citep{shapiro11,gomes17}, and can be implemented by assigning a
vector-clock and/or globally-unique identifier (UID) to each message at the
network layer.} We then refine $P$ to ensure that the properties of \EC (and
\SEC) can be implemented on top of the network, resolving our problem by
discarding degenerate network behaviors.

\subsection{Strong Eventual Consistency}
Another downside of implementing a system which only upholds \EC is that \EC is
merely a liveness guarantee. In particular, \EC does not impose any restriction
on nodes which have received the same set or even sequence of messages. That is,
a pair of replicas which have received the exact set of messages in the exact
same order are not required to return the same value.

\SEC addresses this gap by imposing a safety guarantee in addition to the
previous liveness guarantees in \EC. That is, a system is \SEC when the
following two conditions are met:
\begin{enumerate}
  \item The system is \EC, per above guidelines.
  \item \emph{Strong convergence}. Any pair of replicas which have received the
    same set of messages must return the same value when queried immediately.
    \[
      \forall r_1, r_2.\, (\textsf{delivered}~r_1) = (\textsf{delivered}~r2)
        \Rightarrow q(r_1) = q(r_2)
    \]
\end{enumerate}

That is, it is the strong convergence property of \SEC that distinguishes it
from \EC. On top of \EC, strong convergence is only a moderate safety
restriction. In particular, it imposes no requirements on replicas which have
not received the same sequence or even set of updates. So, unlike strong
distributed consensus algorithms like Paxos or Raft which are fully
linearizable~\citep{lamport98,ongaro14}, \SEC allows certain replicas to be
``behind.'' That is, a replica which hasn't yet received all relevant updates in
the system is allowed to return an earlier version of the computation.

Informally, this means that replicas in the system are allowed to temporarily
diverge from the state of the overall computation. As soon as no more updates
are sent to the system, property (1) of \EC requires that all replicas will
\emph{eventually} converge to a uniform view of the computation.

\section{state-based \CRDTs}
\label{sec:state-based-crdts}

Now that we have discussed \EC and \SEC, we will turn our attention to datatypes
that implement these consistency models. \CRDTs are a common way to implement
the consistency requirements in \SEC. So, we begin with a discussion of
state-based \CRDTs from their inception in~\citet{shapiro11}. A state-based
\CRDT is a 5-tuple $(S, s^0, q, u, m)$. An individual replica of a state-based
\CRDT is at some state $s^i \in S$ for $i \ge 0$, and is initially $s^0$. The
value may be queried by any client or other replica by invoking $q$. It may be
updated with $u$, which has a unique type per \CRDT object. Finally, $m$ merges
the state of some other remote replica.  Neither $q$ nor $u$ have pre-determined
types, per-se, rather they are implementation specific. We discuss a portfolio
of examples to illustrate this point in Chapter~\ref{chap:crdt-instantiations}.

Crucially, the states of a given state-based \CRDT form a partially-ordered set
$\langle S, \sqsubseteq \rangle$. This poset is used to form a join
semi-lattice, where any finite subset of elements has a natural least
upper-bound. Consider two elements $s^m, s^n \in S$. The least upper-bound
$s = s^m \sqcup s^n$ is given as:
\[
  \forall s'.\; s' \sqsupseteq s^m, s^n \Rightarrow
    s^m \sqsubseteq s \land
    s^n \sqsubseteq s \land
    s \sqsubseteq s'
\]
In other words, a $s = s^m \sqcup s^n$ is a least upper-bound of $s^m$ and $s^n$
if it is the smallest element that is at least as large as both $s^m$ and $s^n$.

For now, we set aside $q$ and $u$, and turn our attention towards the merging
function $m$. $m$ resolves the states of two \CRDTs into a new state, which is
then assigned at the replica performing the merge. Given a suitable set of
states which forms a lattice, we assume that:
\[
  m(s_1, s_2) = s_1 \sqcup s_2
\]
for some join semi-lattice with join operation $\sqcup$, and that whenever a
\CRDT replica $r_1$ at state $s_1$ receives an update from another replica
$r_2$ at state $s_2$, that $r_1$ attains a new state $s_1' = m(s_1, s_2)$.
This process, in addition to each replica periodically broadcasting an update
which contains its current state, is carried on continually, and $m$ is
invoked whenever a new state is received. That is, each replica is evolving
over time in response to outside instruction, and in turn these updates cause
internal state transitions, which themselves cause those new states to be
broadcasted and eventually joined at every other replica.

The $\sqcup$ operator has three mathematical properties that make it an
appealing choice for joining states together as in $m$. These are its
\emph{commutativity}, \emph{associativity}, and \emph{idempotency}. That is, for
any states $s_1$, $s_2$, and $s_3$, that:
\begin{itemize}
  \item The operator is \emph{commutative}, i.e., that $s_1 \sqcup s_2 = s_2
    \sqcup s_1$, or that order does not matter.
  \item The operator is \emph{associative}, i.e., that $s_1 \sqcup (s_2 \sqcup
    s_3) = (s_1 \sqcup s_2) \sqcup s_3$, or that grouping of arguments does not
    matter.
  \item Finally, the operator is \emph{idempotent}, i.e., that $s_1 \sqcup s_1 =
    s_1$, or that repeated updates reach a fixed point.
\end{itemize}

These mathematical properties correspond to real-world constraints that often
arise naturally in the area of distributed systems. Take, for example, that
messages may occur out of order. This often happens in, for example, UDP (User
Datagram Protocol) networks, where the received datagrams are not guaranteed to
be in the order that they were sent. Because $\sqcup$ is commutative, replicas
joining the updates of other replicas do not need to receive those updates in
order, because the result of $s_1 \sqcup s_2$ is the same as $s_2 \sqcup s_1$.
That is, it does not matter which of two updates from another replica arrives
first, because the result is the same no matter in which order they are
delivered.

For concreteness, say that we have two replicas, $r_1$ and $r_2$. $r_1$
initially begins at state $s$, and $r_2$ progresses through states $s_1, \ldots,
s_n$ for $n > 0$. We then see that it does not matter the order in which these
updates are delivered to $r_1$. Suppose that we have a mapping $\pi : [n] \to
[n]$ which maps the true order of a state $s_i$ to the order in which it was
delivered. Then, we can see that the choice of $\pi$ is arbitrary, because:
\[
  s \gets s \sqcup (s_{\pi(1)} \sqcup \cdots \sqcup s_{\pi(n)})
\]
for any choice of $\pi$, because
\[
  s_{\pi(1)} \sqcup \cdots \sqcup s_{\pi(n)} = s_1 \sqcup \cdots \cdots s_n
\]
which follows from the fact that $\sqcup$ is commutative. This can be shown
inductively on the number of updates, $n$, given the commutativity of $\sqcup$.

Next, it is often common for packets to be duplicated in transit over a network.
That is, even though a packet may be sent from a source only once, it may be
received by a recipient on the same network multiple times. For this, the
idempotency of $\sqcup$ comes in handy: no matter how many times a state is
broadcast from an evolving replica, any other replica on the network will
tolerate that set of messages, because it only requires the message to be
delivered once. Any additional duplicates are merged in without changing the
state.

Finally, associativity is an appealing property, too, although its applications
are both less immediate and less often-used in this thesis. Suppose that several
replicas of a state-based \CRDT reside on a network with, say, high latency, or
it is otherwise undesirable to send more messages on the network than is
necessary. Because associativity implies that the grouping of updates is
arbitrary, a replica can maintain a \textit{set} of pending updates, and
periodically\footnote{``Periodically'' is arbitrary and is left up to the
implementation, but it would be easy to imagine that this could be interpreted
as whenever the set reaches a certain size, and/or after a certain amount of
time has passed since flushing the set of pending updates.} send that set to
other replicas by first folding $\sqcup$ over it and sending a single update.

\section{op-based \CRDTs}
\label{sec:op-based-crdts}

Operation-based (op-based) \CRDTs, like their state-based counterparts have
internal states that form a semi-lattice. However, their communication styles
differ fundamentally: op-based \CRDTs communicate \textit{operations} that
indicate a kind of update to be applied locally, instead of the \textit{result}
of that update (as is the case in state-based \CRDTs).

This section discusses the original op-based \CRDTs and pays brief attention to
some newer specifications such as \textit{pure op-based \CRDTs}~\citep{shapiro11,
baquero17}.

An op-based \CRDT is a $6$-tuple $(S, s^0, q, t, u, P)$. As in
Section~\ref{sec:state-based-crdts}, $S$, $s^0$, and $q$, retain their
original meaning (that is, the state set, an initial state, and a query
function).  In op-based \CRDTs, the pair $(t,u)$ takes the place of the $m$
merging function from state-based \CRDTs. $t$ and $u$ correspond to
\textit{prepare-update} and \textit{effect-update}, respectively. When an update
is made by a caller (say, for example, incrementing the value of an op-based
\CRDT counter), it is done in two phases~\citep{shapiro11}:
\begin{enumerate}
  \item First, the \textit{prepare-update} implementation $t$ is applied at the
    replica receiving the update. $t$ is side-effect free, and prepares a
    representation of the operation about to take place.
  \item Next, the \textit{effect-update} implementation $u$ is applied at the
    local replica, causing the desired update to take effect.
  \item Finally, the representation from $t$ is sent to all other replicas using
    the delivery relation $P$, at which point they, too, apply the
    representation of the effect using $u$.
\end{enumerate}

This is the critical distinction between op- and state-based CRDTS:
state-based \CRDTs propagate their state by applying a local update and taking
advantage of the lattice structure of their state-space in order to define a
convenient merge function. On the other hand, op-based \CRDTs propagate their
state by sending the \textit{representation} of an update to other replicas as
an instruction. This critical juncture translates into a corresponding
relaxation in the operation $(t, u)$, which is that unlike the state-based \CRDTs
whose $m$ must be commutative, associative, and idempotent, and op-based \CRDT
implementation of $(t, u)$ need only be commutative.

To explain why, we briefly restate the definition of a causal history for
op-based \CRDTs:

\begin{definition}[op-based Causal History~\citep{shapiro11}]
An object's casual history $C = \{ c_1, \ldots, c_n \}$ is defined as follows.
Initially, $c_i^0 = \emptyset$ for all $i \in \mathcal{I}$. If the $k$th method
execution is idempotent (that is, it is either $q$ or $t$), then the causal
history remains unchanged in the $k$th step, i.e., that $c_i^k = c_i^{k-1}$. If
the execution at $k$ is non-idempotent (i.e., it is $u$), then $c_i^{k} =
c_i^{k-1} \cup \{ u_i^k(\cdot) \}$.
\end{definition}

Causal history of an op-based \CRDT is defined based on the
\textit{happens-before} relation $\to$ as follows. An update $(t,u)$ happens
before $(t',u')$ (i.e., that $(t, u) \to (t', u')$) iff $u \in c_{j}^{K_j(t')}$
if $K_j$ is the injective mapping from operation to execution time. Shapiro and
his co-authors go on to describe a sufficient definition for the commutativity
of $(t,u)$ in op-based \CRDTs. In effect, they say that two pairs $(t,u)$ and
$(t',u')$ commute if and only if for any reachable state $s \in S$ the effect of
applying them in either order is the same. That is, $s \circ u \circ u' \equiv s
\circ u' \circ u$.

They claim that having commutativity for concurrent operations as well as an
in-order delivery relation $P$ for comparable updates is sufficient to prove
that op-based \CRDTs achieve Strong Eventual Consistency.

\section{$\delta$-state \CRDTs}
In this section, we describe the refinement of \CRDTs that is the interest and
focus of the body of this thesis. That is the $\delta$-state \CRDT, as described
in~\citet{almedia18}. In their original work, Almeida and his co-authors
describe $\delta$-state \CRDTs as:
\begin{quote}
...ship[ping] a \textit{representation of the effect} of recent update operations
on the state, rather than the whole state, while preserving the idempotent
nature of \textit{join}
\end{quote}

We will present an example of the $\delta$-state \CRDT in a below section. For
now, we focus on the background material necessary to contextualize
$\delta$-state \CRDTs. This refinement can be thought of as taking ideas from
both state- and op-based \CRDTs to mediate some of the trade-offs described
above. Like a state-based \CRDT, $\delta$-state based \CRDTs have both internal
states and message payloads that form a join semi-lattice. This endows the
$\delta$-state \CRDT with a commutative, associative, and idempotent \emph{join}
operator, as before. Likewise, this means that the $\delta$-state \CRDT supports
weak delivery semantics, such as delayed, dropped,\footnote{In this thesis, we
consider dropped messages as having been delayed for an infinite amount of time,
allowing us to reason about a smaller set of delivery semantics.} reordered,
and duplicated message delivery.

Unlike a state-based \CRDT, however, $\delta$-state \CRDTs do not send their
internal state $s^k$ after an update at time $k-1$. We require that these states
have natural representations of their \emph{updates} which do not require
sending the full state to all other replicas. In many circumstances, these
updates can often be represented as ``smaller'' items within the set of all
possible reachable states. For example, in a \CRDT which supports adding to a
set of items, a $\delta$-mutation may be the singleton set containing the
newly-added item, whereas a traditional state-based \CRDT may include the full
set.

This means that:
\begin{itemize}
  \item $\delta$-state \CRDTs support the same weak requirements from the network
    as ordinary state-based \CRDTs. That is, they support dropping, duplicating,
    reordering, and delaying of messages.
  \item $\delta$-state \CRDTs have similarly low-overhead of message size as
    op-based \CRDTs.
\end{itemize}
On the converse, $\delta$-state \CRDTs do not:
\begin{itemize}
  \item ...have potentially large payload size, as state-based \CRDTs are prone
    to have.
  \item ...require a strong delivery semantics $P$ that ensures ordered,
    at-most-once delivery as op-based \CRDTs do.
\end{itemize}
Said otherwise, $\delta$-state \CRDTs have the relative strengths of both state-
and op-based \CRDTs without their respective drawbacks. This makes them an area
of interest, and they are the subject to which we dedicate the remainder of this
thesis.
