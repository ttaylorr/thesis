\chapter{Background}
\label{chap:background}

This chapter outlines the preliminary information necessary to contextualize the
remainder of this thesis for readers unfamiliar with existing \CRDT research.
Here we motivate conflict-free replicated datatypes (\CRDTs), formalize state-
and op-based variants of \CRDTs, and present examples of common instantiations.
Finally, we conclude with a discussion of the different levels of consistency
guarantees that each \CRDT variant offers, and rationalize which levels of
consistency are appealing in certain situations.

\section{Motivation}
Conflict-free Replicated Datatypes (\CRDTs) are a way to store several copies of
a data-structure on multiple computers which form a distributed system. Each
participant in the system can make modifications to the datatype without
the need for explicit coordination with other participants. Their
implementations are designed so that coordination-free updates which may
conflict with one another always have a deterministic resolution. This allows
multiple participants to query and modify their \emph{view} of the replicated
datatype, without the traditional overhead and implementation burden that more
stringent replication algorithms require.

Here, we'll discuss four variants of \CRDTs: state-based, op-based, pure
op-based, and $\delta$-state based. Each of these variants achieve a consistent
value by the use of different message types, and each likewise requires a
different set of delivery semantics. In this chapter, we identify $\delta$-state
\CRDTs as achieving an appealing set of trade-offs among each of the four
variants. We restate that they are able to achieve Strong Eventual Consistency
(the best reasonably-achievable consistency guarantee for most \CRDT
applications) while maintaining both:
\begin{itemize}
  \item A relatively small payload size, as is the benefit of op-based \CRDTs,
    and
  \item Relatively weak delivery semantics, as is the benefit of stat-based
    \CRDTs.
\end{itemize}

\section{Coordinated Replication}
In a distributed system, it is common for more than one participant to need to
have a \textit{view} of the same data. For example, multiple nodes may need to
have access to the same internal data structures necessary to execute some
computation. When a piece of data is shared among many participants in a system,
we say that that data is \textit{replicated}.

However, saying only that some data is ``replicated'' is under-specified. For
example: how often is that data updated among multiple participants? How does
that data behave when multiple participants are modifying it concurrently? Do
all participants always have the same view of the data, or are there temporary
divergences among the participants in the system?

It turns out that the answer to the last question is of paramount importance.
Traditionally speaking, in a distributed system, all participants have an
identical replica of any piece of shared data at all times. That is, at no
moment in time will there be a replica that could atomically compare its
replicated value for some data with any other replica for equality and disagree.
Said otherwise, all replicated values are equal everywhere all at once. This is
an appealing property to say the least, because it allows system designers to
conceptually treat a distributed system as a single unit of computation. That
is, if all replicas maintain the same memory, it is conceptually as if one whole
machine is being replicated many times.

That being said, upholding this requirement is not a straightforward task. An
immediate question arises which is: who coordinates when updates to a piece of
data are replicated to other participants in the system? What happens when the
coordinator becomes unresponsive, or otherwise misbehaves? Who is responsible
for electing a new participant to take over the coordination duties of the
participant which was no longer able to fulfill them?

\section{Distributed Consensus Algorithms}

This gives rise to the area of consensus algorithms. A consensus algorithm,
broadly speaking, is a routine which multiple participants follow in order to
agree on a shared value.

We first state briefly the properties that an algorithm must have to solve
distributed consensus:
\begin{definition}[Distributed Consensus Algorithm, \citep{howard19}]
  \label{def:consensus}
  An algorithm is said to solve distributed consensus if it has the following
  three safety requirements:
  \begin{enumerate}
    \item \emph{Non-triviality}: The decided value must have been proposed by a
      participant.
    \item \emph{Safety}: Once a value has been decided, no other value will be
      decided.
    \item \emph{Safe learning}: If a participant learns a value, it must learn
      the decided value.
  \end{enumerate}
  In addition, it must satisfy the following two progress requirements:
  \begin{enumerate}
    \item \emph{Progress}: Under previously agreed-upon liveness conditions, if
      a value is proposed by a participant, then a value is eventually decided.
    \item \emph{Eventual learning}: Under the same conditions as above, if a
      value is decided, then that value must be eventually learned.
  \end{enumerate}
\end{definition}

We summarize the two most popular and often-implemented algorithms for
distributed consensus~\citep{howard20}:

\paragraph{Paxos}
The most popular algorithm in this field is Paxos~\citep{lamport98}. Broadly
speaking, values (corresponding to changing the value of a replicated piece of
data) are \textit{proposed}, \textit{accepted}, and \textit{learned} by
participants in the system. This process is coordinated by an elected
\textit{leader}, which is responsible for communicating with other participants
in order to drive the process forward. When a leader becomes unresponsive, other
participants in the system begin an election process to replace the leader, and
do so when a majority of participants (and the new leader) acknowledge the
change. In practice, \emph{multi-round} Paxos--that is, a repeated instantiation
of \emph{single-decree} Paxos--takes the place of a ``log'' of agreed-upon
events.

\paragraph{Raft}
Lest we omit another often-implemented consensus algorithm, we briefly discuss
the Raft consensus algorithm as well~\citep{ongaro14}. Raft emphasizes
understandability in its design, and ``separates the key elements of consensus''
by silo-ing replication, leader election, and safety into different sub-problems
at the design level. An execution of Raft consists of several \textit{terms}. To
begin each term, an election is held in order to determine a \textit{leader}.
Once elected, the leader is responsible for disseminating updates to each
replicas copy of the \textit{log}. Conflicting log entries are always resolved
in favor of the leader's copy. Finally, Raft enforces safety by imposing
additional restrictions on the behavior of a term such that the log replication
strategy is proven to be safe. This is argued in~\citep{ongaro14} and
mechanically verified in~\citep{wilcox15}.

\subsection{Safety in Distributed Consensus Algorithms}

\sloppy
Both Raft and Paxos are notoriously difficult to implement correctly in
practice~\citep{howard20}. Distributed consensus algorithms are often the
subject of undergraduate-level courses in networks and distributed systems.
Often, commercially-available implementations of these algorithms are built into
off the shelf solutions~\citep{etcd,zookeeper}.

Therefore, it is a natural question to ask why these algorithms are so
notoriously difficult to implement in practice. Individually, the properties in
Definition~\ref{def:consensus} seem reasonably in their own right. We propose
that it is the safety property (that once a value has been decided, no other
values will be decided) that makes implementing these algorithms correctly so
difficult in practice. In essence, coordinating the proposals individual
participants submit is the central difficulty of these algorithms.

Conflict-free Replicated Datatypes (\CRDTs) are a natural answer to this
question. By allowing participants to temporarily diverge from the state of the
overall computation (c.f., the second property of
Definition~\ref{def:eventual-consistency}), \CRDTs allow replicas to violate the
safety property of Definition~\ref{def:consensus}. By giving up the immediacy
and permanence that the safety properties of a traditional distributed consensus
algorithm, \CRDTs allow for a dramatically lower implementation burden in
practice, and are substantially easier to reason about.

\section{state-based \CRDTs}
\label{sec:state-based-crdts}
We begin with a discussion of state-based \CRDTs from their inception
in~\citep{shapiro11}. A state-based \CRDT is a 5-tuple $(S, s^0, q, u, m)$. An
individual replica of a state-based \CRDT is at some state $s^i \in S$ for $i \ge
0$, and is initially $s^0$. The value may be queried by any client or other
replica by invoking $q$. It may be updated with $u$, which has a unique type per
\CRDT object. Finally, $m$ merges the state of some other remote replica.
Neither $q$ nor $u$ have pre-determined types, per-se, rather they are
implementation specific. We discuss a portfolio of examples to illustrate this
point in Section~\ref{sec:crdt-portfolio}.

Crucially, the states of a given state-based \CRDT form a partially-ordered set
$\langle S, \sqsubseteq \rangle$. This poset is used to form a join
semi-lattice, where any finite subset of elements has a natural least
upper-bound. Consider two elements $s^m, s^n \in S$. The least upper-bound
$s = s^m \sqcup s^n$ is given as:
\[
  \forall s'.\; s' \sqsupseteq s^m, s^n \Rightarrow
    s^m \sqsubseteq s \land
    s^n \sqsubseteq s \land
    s \sqsubseteq s'
\]
In other words, a $s = s^m \sqcup s^n$ is a least upper-bound of $s^m$ and $s^n$
if it is the smallest element that is at least as large as both $s^m$ and $s^n$.

For now, we set aside $q$ and $u$, and turn our attention towards the merging
function $m$. $m$ resolves the states of two \CRDTs into a new state, which is
then assigned at the replica performing the merge. Given a suitable set of
states which forms a lattice, we assume that:
\[
  m(s_1, s_2) = s_1 \sqcup s_2
\]
for some join semi-lattice with join operation $\sqcup$, and that whenever a
\CRDT replica $r_1$ at state $s_1$ receives an update from another replica
$r_2$ at state $s_2$, that $r_1$ attains a new state $s_1' = m(s_1, s_2)$.
This process, in addition to each replica periodically broadcasting an update
which contains its current state, is carried on continually, and $m$ is
invoked whenever a new state is received. That is, each replica is evolving
over time in response to outside instruction, and in turn these updates cause
internal state transitions, which themselves cause those new states to be
broadcasted and eventually joined at every other replica.

The $\sqcup$ operator has three mathematical properties that make it an
appealing choice for joining states together as in $m$. These are its
\emph{commutativity}, \emph{associativity}, and \emph{idempotency}. That is, for
any states $s_1$, $s_2$, and $s_3$, that:
\begin{itemize}
  \item The operator is \emph{commutative}, i.e., that $s_1 \sqcup s_2 = s_2
    \sqcup s_1$, or that order does not matter.
  \item The operator is \emph{associative}, i.e., that $s_1 \sqcup (s_2 \sqcup
    s_3) = (s_1 \sqcup s_2) \sqcup s_3$, or that grouping of arguments does not
    matter.
  \item Finally, the operator is \emph{idempotent}, i.e., that $s_1 \sqcup s_1 =
    s_1$, or that repeated updates reach a fixed point.
\end{itemize}

These mathematical properties correspond to real-world constraints that often
arise naturally in the area of distributed systems. Take, for example, that
messages may occur out of order. This often happens in, for example, UDP (User
Datagram Protocol) networks, where the received datagrams are not guaranteed to
be in the order that they were sent. Because $\sqcup$ is commutative, replicas
joining the updates of other replicas do not need to receive those updates in
order, because the result of $s_1 \sqcup s_2$ is the same as $s_2 \sqcup s_1$.
That is, it does not matter which of two updates from another replica arrives
first, because the result is the same no matter in which order they are
delivered.

For concreteness, say that we have two replicas, $r_1$ and $r_2$. $r_1$
initially begins at state $s$, and $r_2$ progresses through states $s_1, \ldots,
s_n$ for $n > 0$. We then see that it does not matter the order in which these
updates are delivered to $r_1$. Suppose that we have a mapping $\pi : [n] \to
[n]$ which maps the true order of a state $s_i$ to the order in which it was
delivered. Then, we can see that the choice of $\pi$ is arbitrary, because:
\[
  s \gets s \sqcup (s_{\pi(1)} \sqcup \cdots \sqcup s_{\pi(n)})
\]
for any choice of $\pi$, because
\[
  s_{\pi(1)} \sqcup \cdots \sqcup s_{\pi(n)} = s_1 \sqcup \cdots \cdots s_n
\]
which follows from the fact that $\sqcup$ is commutative. This can be shown
inductively on the number of updates, $n$, given the commutativity of $\sqcup$.

Next, it is often common for packets to be duplicated in transit over a network.
That is, even though a packet may be sent from a source only once, it may be
received by a recipient on the same network multiple times. For this, the
idempotency of $\sqcup$ comes in handy: no matter how many times a state is
broadcast from an evolving replica, any other replica on the network will
tolerate that set of messages, because it only requires the message to be
delivered once. Any additional duplicates are merged in without changing the
state.

Finally, associativity is an appealing property, too, although its applications
are both less immediate and less often-used in this thesis. Suppose that several
replicas of a state-based \CRDT reside on a network with, say, high latency, or
it is otherwise undesirable to send more messages on the network than is
necessary. Because associativity implies that the grouping of updates is
arbitrary, a replica can maintain a \textit{set} of pending updates, and
periodically\footnote{``Periodically'' is arbitrary and is left up to the
implementation, but it would be easy to imagine that this could be interpreted
as whenever the set reaches a certain size, and/or after a certain amount of
time has passed since flushing the set of pending updates.} send that set to
other replicas by first folding $\sqcup$ over it and sending a single update.

\section{op-based \CRDTs}
\label{sec:op-based-crdts}

Operation-based (op-based) \CRDTs, like their state-based counterparts have
internal states that form a semi-lattice. However, their communication styles
differ fundamentally: op-based \CRDTs communicate \textit{operations} that
indicate a kind of update to be applied locally, instead of the \textit{result}
of that update (as is the case in state-based \CRDTs).

This section discusses the original op-based \CRDTs and pays brief attention to
some newer specifications such as \textit{pure op-based \CRDTs}~\citep{shapiro11,
baquero17}.

An op-based \CRDT is a $6$-tuple $(S, s^0, q, t, u, P)$. As in
Section~\ref{sec:state-based-crdts}, $S$, $s^0$, and $q$, retain their
original meaning (that is, the state set, an initial state, and a query
function).  In op-based \CRDTs, the pair $(t,u)$ takes the place of the $m$
merging function from state-based \CRDTs. $t$ and $u$ correspond to
\textit{prepare-update} and \textit{effect-update}, respectively. When an update
is made by a caller (say, for example, incrementing the value of an op-based
\CRDT counter), it is done in two phases~\citep{shapiro11}:
\begin{enumerate}
  \item First, the \textit{prepare-update} implementation $t$ is applied at the
    replica receiving the update. $t$ is side-effect free, and prepares a
    representation of the operation about to take place.
  \item Next, the \textit{effect-update} implementation $u$ is applied at the
    local replica, causing the desired update to take effect.
  \item Finally, the representation from $t$ is sent to all other replicas using
    the delivery relation $P$, at which point they, too, apply the
    representation of the effect using $u$.
\end{enumerate}

This is the critical distinction between op- and state-based CRDTS:
state-based \CRDTs propagate their state by applying a local update and taking
advantage of the lattice structure of their state-space in order to define a
convenient merge function. On the other hand, op-based \CRDTs propagate their
state by sending the \textit{representation} of an update to other replicas as
an instruction. This critical juncture translates into a corresponding
relaxation in the operation $(t, u)$, which is that unlike the state-based \CRDTs
whose $m$ must be commutative, associative, and idempotent, and op-based \CRDT
implementation of $(t, u)$ need only be commutative.

To explain why, we briefly restate the definition of a causal history for
op-based \CRDTs:

\begin{definition}[op-based Causal History~\citep{shapiro11}]
An object's casual history $C = \{ c_1, \ldots, c_n \}$ is defined as follows.
Initially, $c_i^0 = \emptyset$ for all $i \in \mathcal{I}$. If the $k$th method
execution is idempotent (that is, it is either $q$ or $t$), then the causal
history remains unchanged in the $k$th step, i.e., that $c_i^k = c_i^{k-1}$. If
the execution at $k$ is non-idempotent (i.e., it is $u$), then $c_i^{k} =
c_i^{k-1} \cup \{ u_i^k(\cdot) \}$.
\end{definition}

Causal history of an op-based \CRDT is defined based on the
\textit{happens-before} relation $\to$ as follows. An update $(t,u)$ happens
before $(t',u')$ (i.e., that $(t, u) \to (t', u')$) iff $u \in c_{j}^{K_j(t')}$
if $K_j$ is the injective mapping from operation to execution time. Shapiro and
his co-authors go on to describe a sufficient definition for the commutativity
of $(t,u)$ in op-based \CRDTs. In effect, they say that two pairs $(t,u)$ and
$(t',u')$ commute if and only if for any reachable state $s \in S$ the effect of
applying them in either order is the same. That is, $s \circ u \circ u' \equiv s
\circ u' \circ u$.

They claim that having commutativity for concurrent operations as well as an
in-order delivery relation $P$ for comparable updates is sufficient to prove
that op-based \CRDTs achieve Strong Eventual Consistency.

\subsection{Pure op-based \CRDTs}

For completeness, we briefly discuss the work of Baquero and his co-authors in
developing \emph{pure} op-based \CRDTs~\citep{baquero17}. Although we devote some
time to their discussion, we do not investigate pure op-based \CRDTs further in
this thesis. Pure op-based \CRDTs address a highly-interpretable definition of
$t$ and $u$ to restrict the payload size of the messages that these operations
generate and require, respectively. In particular, they point out that op-based
\CRDTs may behave as if they were state-based \CRDTs when $t$ encodes the state
$s^k$ and $u$ acts like a full join on the lattice $S$.

A particular advantage of op-based \CRDTs over their state-based counter-parts is
that they are generally believed to require less overall bandwidth to
communicate updates. The key trade-off is that op-based \CRDTs require a much
stronger delivery semantics $P$ (in particular, $P$ must provide in-order and
at-most-once delivery of messages) in order to attain Strong Eventual
Consistency. Of course, treating an op-based \CRDT as a state-based one
eliminates many of these benefits.

To address this issue, Baquero and his co-authors instead treat the state-space
$S$ of a \emph{pure} op-based \CRDT as a poset of
operations\footnote{Incomparable items are generally interpreted as concurrent
operations performed on a pure op-based \CRDT}, where the state is interpreted as
applying elements in the poset order. They couple this with the additional
restriction that $(t,u)$ can only send operations to other replicas, meaning
that, for instance, they cannot send the entire state of a replica to all other
replicas.

\section{$\delta$-state \CRDTs}
In this section, we describe the refinement of \CRDTs that is the interest and
focus of the body of this thesis. That is the $\delta$-state \CRDT, as described
in~\citep{almedia18}. In their original work, Almeida and his co-authors
describe $\delta$-state \CRDTs as:
\begin{quote}
...ship[ping] a \textit{representation of the effect} of recent update operations
on the state, rather than the whole state, while preserving the idempotent
nature of \textit{join}
\end{quote}

We will present an example of the $\delta$-state \CRDT in a below section. For
now, we focus on the background material necessary to contextualize
$\delta$-state \CRDTs. This refinement can be thought of as taking ideas from
both state- and op-based \CRDTs to mediate some of the trade-offs described
above. Like a state-based \CRDT, $\delta$-state based \CRDTs have both internal
states and message payloads that form a join semi-lattice. This endows the
$\delta$-state \CRDT with a commutative, associative, and idempotent \emph{join}
operator, as before. Likewise, this means that the $\delta$-state \CRDT supports
weak delivery semantics, such as delayed, dropped,\footnote{In this thesis, we
consider dropped messages as having been delayed for an infinite amount of time,
allowing us to reason about a smaller set of delivery semantics.} reordered,
and duplicated message delivery.

Unlike a state-based \CRDT, however, $\delta$-state \CRDTs do not send their
internal state $s^k$ after an update at time $k-1$. We require that these states
have natural representations of their \emph{updates} which do not require
sending the full state to all other replicas. In many circumstances, these
updates can often be represented as ``smaller'' items within the set of all
possible reachable states. For example, in a \CRDT which supports adding to a
set of items, a $\delta$-mutation may be the singleton set containing the
newly-added item, whereas a traditional state-based \CRDT may include the full
set.

This means that:
\begin{itemize}
  \item $\delta$-state \CRDTs support the same weak requirements from the network
    as ordinary state-based \CRDTs. That is, they support dropping, duplicating,
    reordering, and delaying of messages.
  \item $\delta$-state \CRDTs have similarly low-overhead of message size as
    op-based \CRDTs.
\end{itemize}
On the converse, $\delta$-state \CRDTs do not:
\begin{itemize}
  \item ...have potentially large payload size, as state-based \CRDTs are prone
    to have.
  \item ...require a strong delivery semantics $P$ that ensures ordered,
    at-most-once delivery as op-based \CRDTs do.
\end{itemize}
Said otherwise, $\delta$-state \CRDTs have the relative strengths of both state-
and op-based \CRDTs without their respective drawbacks. This makes them an area
of interest, and they are the subject to which we dedicate the remainder of this
thesis.

\section{Elementary \CRDT instantiations}
\label{sec:crdt-portfolio}
In this section, we provide the specification of three common \CRDT instantiations
for three of the four aforementioned \CRDT refinements. In particular, we show
instantiations of the Grow-Only Counter (G-Counter), Positive-Negative Counter
(PN-Counter), and the Grow-Only Set (G-Set) for state-, op-, and
$\delta$-state-based \CRDTs. In each of the below, we assume that $\mathcal{I}$
refers to the set of node identifiers corresponding to the active
replicas.\footnote{In this thesis, we consider $\mathcal{I}$ to be fixed during
execution; that is, we do not support addition or deletion of replicas.}

\subsection{Example: Grow-Only Counter}
\label{sec:example-gcounter}

\paragraph{State-based GCounter}
The G-Counter supports two very simple operations: \textsf{inc}, and query. When
\textsf{inc} is invoked, the counter updates its internal state to increment the
queried value by one. When query is invoked, the counter returns a number which
is at least as large as the number of times that \textsf{inc} has been invoked
\textit{at that replica}.

This is our first example of \SEC, where replicas that are ``behind'', i.e.,
that have not received all updates from all other replicas, are not guaranteed
to reflect the same value upon being queried.\footnote{Perhaps these messages
were delayed or dropped in transit, or otherwise the other replicas have not
broadcast their updates yet. The latter is uncommon in traditional state-based
\CRDTs, but is an often-used operation in variants of state-based \CRDTs
(including $\delta$-state \CRDTs) where updates are bundled into
\emph{intervals} which are sent in a way that preserves causality of updates.}
Concretely, suppose that an \textsf{inc} has occurred at at least one other
replica which has not yet broadcast its updated state. The replica being queried
will have therefore not yet merged the updated state from the replica(s)
receiving \textsf{inc},\footnote{Because we cannot merge updates we do not know
about.} and so those update(s) will not be reflected in the value returned by
querying.

We present a state-based G-Counter \CRDT for concreteness, and then discuss its
definition:

\begin{figure}[H]
  \centering
  \[
    \textsf{G-Counter}_s = \left\{\begin{aligned}
      S &: \mathbb{N}_+^{|\mathcal{I}|} \\
      s^0 &: \left[ 0, 0, \cdots, 0 \right] \\
      q &: \lambda s. \sum_{i \in \mathcal{I}} s(i) \\
      u &: \lambda s,i. s\left\{ i \mapsto s(i) + 1 \right\} \\
      m &: \lambda s_1, s_2. \left\{ \max\left\{ s_1(i), s_2(i) \right\}: i \in \mathsf{dom}(s_1) \cup
      \mathsf{dom}(s_2) \right\}
    \end{aligned}\right.
  \]
  \caption{state-based \textsf{G-Counter} \CRDT}
  \label{fig:state-gcounter}
\end{figure}

Notice that the state space $\mathbb{N}^{|\mathcal{I}|}_+$ does not match the
return type of the query function, $q$, which is simply $\mathbb{N}_+$. One way
to understand this is to consider what might happen to certain interleavings of
updates using this state-space. Consider the following example:

\begin{example}
  \label{example:vector-state-counter}
  Suppose we have two replicas, $r_1$ and $r_2$, and consider the following
  sequence of events:

  \begin{figure}[H]
    \centering
    \includegraphics[width=.5\textwidth]{figures/scalar-state.pdf}
    \caption{An (invalid) state-based GCounter with scalar state.}
  \end{figure}

  \begin{enumerate}
    \item Initially, $q(r_1) = q(r_2) = 0$. Suppose next that one update is
      issued at each replica concurrently. That is, at the same time, we run
      both $u(r_1)$ and $u(r_2)$ so that $q(r_1) = q(r_2) = 1$.
    \item Then, each replica sends its state to the other where it is joined
      with the current state. Here, $r_1$ tells $r_2$ that its state is $1$,
      which $r_2$ joins with its own state, also $1$.
  \end{enumerate}

  Since $\sqcup$ is idemopotent, the state at both replicas remains $1$ until
  further updates are issued. This does not reflect the overall state: without
  further updates, $q(r_1)$ which equals $q(r_2)$ should equal $2$, not $1$.
\end{example}

Example~\ref{example:vector-state-counter} demonstrates the need for a
vector-based counter, or, at least the need for a non-scalar value. In
Figure~\ref{fig:state-gcounter}, we utilize a \emph{vector} counter, which
should be familiar to readers acquainted with \emph{vector
clocks}~\citep{lamport78}.\footnote{Unlike traditional vector clocks, the vector
\emph{counter} only stores in each replica's slot the number of \textsf{inc}
operations performed \emph{at that replica}.} The state-based GCounter maintains
a vector of natural numbers, where each element in the vector corresponds to the
number of \textsf{inc} operations applied at that replica.\footnote{We assume
that there exists some injective mapping $f : \mathcal{I} \to \mathbb{N}$.}

\begin{figure}[H]
  \centering
  \includegraphics[width=.6\textwidth]{figures/vector-state.pdf}
  \caption{A (correct) state-based GCounter with vector state.}
\end{figure}

When an \textsf{inc} is invoked at the $i$th replica, it updates its own state
to increment by one the vector element associated with the $i$th replica, here
denoted $s\{i \mapsto s(i) + 1\}$. Finally, upon receiving an update from
another replica, the pair-wise maximum is taken on each of the vector elements.
Note that this is a commutative, associative, and idempotent operation, and so
it forms the least upper-bound of a lattice of vectors of natural numbers.

\paragraph{op-based GCounter} In the op-based variant of the GCounter, we can
rely on a delivery semantics $P$ which guarantees at-most-once message
delivery.\footnote{That is, the network is allowed to drop, reorder, and delay
messages, but a single message will never be delivered more than once.} From
this, we say that replicas which are ``behind'' have not yet received the set of
all \textsf{inc} operations performed at other replicas. Replicas which are
``behind'' may ``catch up'' when they receive the set of undelivered messages.
However, these replicas never are ``ahead'' of any other replica, i.e., they
never receive a message which doesn't correspond to a single \textsf{inc}
operation at some other replica.

We present now the full definition of the op-based GCounter:

\begin{figure}[H]
  \centering
  \[
    \textsf{G-Counter}_o = \left\{\begin{aligned}
      S &: \mathbb{N}_+ \\
      s^0 &: 0 \\
      q &: \lambda s. s \\
      t &: \textsf{inc} \\
      u &: \lambda s,p. s + 1 \\
    \end{aligned}\right.
  \]
  \caption{op-based \textsf{G-Counter} \CRDT}
\end{figure}

Because replicas are sometimes behind but never ahead, we know that the number
of messages received at any given replica is no greater than the sum of the
number of \textsf{inc} operations performed at other replicas, and the number of
\textsf{inc} operations performed locally. So, the op-based GCounter needs only
to keep track of the number of \textsf{inc} operations it knows about globally,
and this can be done using a single natural number. Hence, $S = \mathbb{N}_+$,
and the bottom state is $0$.

The query operation $q$ is as straightforward as returning the initial state.
The \emph{prepare-update} function $t$ always produces the sentinel
\textsf{inc}, indicating that an increment operation should be performed at the
receiving replica. Finally, $u$ takes a state and an arbitrary
payload\footnote{Unused in the implementation here, since the only operation is
\textsf{inc}.} and returns the successor.

\paragraph{$\delta$-state based GCounter}
We conclude this subsection by turning our attention to the $\delta$-state based
GCounter. We begin first by presenting its full definition:

\begin{figure}[H]
  \centering
  \[
    \textsf{G-Counter}_\delta = \left\{\begin{aligned}
      S &: \mathbb{N}_+^{|\mathcal{I}|} \\
      s^0 &: \left[ 0, 0, \cdots, 0 \right] \\
      q &: \lambda s. \sum_{i \in \mathcal{I}} s(i) \\
      u &: \lambda s,i. \left\{ i \mapsto s(i) + 1 \right\} \\
      m &: \lambda s_1, s_2. \left\{ \max\left\{ s_1(i), s_2(i) \right\}: i \in \mathsf{dom}(s_1) \cup
      \mathsf{dom}(s_2) \right\}
    \end{aligned}\right.
  \]
  \caption{$\delta$-state based \textsf{G-Counter} \CRDT}
\end{figure}

It is worth mentioning the extreme levels of similarity it shares with its
state-based counterpart. Like the state-based GCounter, the $\delta$-state based
GCounter uses the state-space $\mathbb{N}^{|\mathcal{I}|}_+$, and has $s^0 = 0$.
Its query operation and merge are defined identically.

However, unlike the state-based GCounter, the $\delta$-state based GCounter
implements the update function as $\lambda s,i.\, \{ i \mapsto s(i) + 1\}$. That
is, instead of returning the amended map (recall: $s\{ \cdots \}$), the
$\delta$-state based GCounter returns the \emph{singleton map} containing
\emph{only} the updated index. Because of the definition of $m$ (namely, that it
does a pairwise maximum over the \emph{union} of the domains of the two states),
sending the singleton map is equivalent to sending the full map with all other
entries being equal.

This follows from the facts that: (1) the entry being updated has the same
pairwise maximum independent of all other entries in the map, and (2) the
pairwise maximum of all \emph{other} entries does not depend on the updated
entry. So, taking the pairwise maximum of any state with the singleton map
containing one updated value is equivalent to taking the pairwise maximum with
our own state modulo one updated value. $m$ is therefore referred to as a
\emph{$\delta$-mutator}, and the value it returns is an \emph{$\delta$
mutation}~\citep{almedia18}.

This principle of sending \emph{smaller} states (the $\delta$ mutations) which
communicate only the \emph{changed} information is a general principle which
we will return to in the remaining two examples.

\subsection{Example: PN-Counter}
\label{sec:example-pncounter}

\begin{figure}[H]
  \[
    \textsf{PN-Counter}_s = \left\{\begin{aligned}
      S &: \mathbb{N}_+^{|\mathcal{I}|} \times \mathbb{N}_+^{|\mathcal{I}|} \\
      s^0 &: \left[ 0, 0, \cdots, 0 \right] \times \left[ 0, 0, \cdots, 0 \right] \\
      q &: \lambda s. \sum_{i \in \mathcal{I}} (\fst~s)(i) - \sum_{i \in
        \mathcal{I}} (\snd~s)(i) \\
      u &: \lambda s,(i,op). \begin{cases}
             (\fst~s)\left\{ i \mapsto (\fst~s)(i) + 1 \right\} \times (\snd~s) & \text{if $op=+$} \\
             (\fst~s) \times (\snd~s)\left\{ i \mapsto (\snd~s)(i) + 1 \right\} & \text{if $op=-$} \\
           \end{cases} \\
      m &: \lambda s_1, s_2.\, \begin{aligned}
             &\left\{ \max\{ i_1, i_2 \} : i \in \mathsf{dom}((\fst~s_1) \cup (\fst~s_1)) \right\} \\
             \times &\left\{ \max\{ i_1, i_2 \} : i \in \mathsf{dom}((\snd~s_1) \cup (\snd~s_1)) \right\}
           \end{aligned}
    \end{aligned}\right.
  \]
  \caption{state-based \textsf{PN-Counter} \CRDT}
\end{figure}

\begin{figure}[H]
  \centering
  \[
    \textsf{PN-Counter}_o = \left\{\begin{aligned}
      S &: \mathbb{N}_+ \times \mathbb{N}_+  \\
      s^0 &: 0 \times 0 \\
      q &: \lambda s. (\fst~s) - (\snd~s) \\
      t &: \lambda i, \mathsf{op}. (i, \mathsf{op}),\quad \mathsf{op} \in \{
        \mathsf{inc}, \mathsf{dec} \} \\
      u &: \lambda s,\mathsf{op}. \begin{cases}
             (\fst~s) + 1 \times (\snd~s) & \text{if $\mathsf{op} = \mathsf{inc}$} \\
             (\fst~s) \times (\snd~s) + 1 & \text{if $\mathsf{op} = \mathsf{dec}$} \\
           \end{cases} \\
    \end{aligned}\right.
  \]
  \caption{op-based \textsf{PN-Counter} \CRDT}
\end{figure}

\begin{figure}[H]
  \[
    \textsf{PN-Counter}_\delta = \left\{\begin{aligned}
      S &: \mathbb{N}_+^{|\mathcal{I}|} \times \mathbb{N}_+^{|\mathcal{I}|} \\
      s^0 &: \left[ 0, 0, \cdots, 0 \right] \times \left[ 0, 0, \cdots, 0 \right] \\
      q^\delta &: \lambda s. \sum_{i \in \mathcal{I}} (\fst~s)(i) - \sum_{i \in
        \mathcal{I}} (\snd~s)(i) \\
      u^\delta &: \lambda s,(i,op). \begin{cases}
             \{ i \mapsto 1 \} \times \emptyset & \text{if $op=+$} \\
             \emptyset \times \{ i \mapsto 1 \} & \text{if $op=-$} \\
           \end{cases} \\
      m^\delta &: \lambda s_1, s_2.\, \begin{aligned}
             &\left\{ \max\{ i_1, i_2 \} : i \in \mathsf{dom}((\fst~s_1) \cup (\fst~s_1)) \right\} \\
             \times &\left\{ \max\{ i_1, i_2 \} : i \in \mathsf{dom}((\snd~s_1) \cup (\snd~s_1)) \right\}
           \end{aligned}
    \end{aligned}\right.
  \]
  \caption{$\delta$-state based \textsf{PN-Counter} \CRDT}
\end{figure}


\subsection{Example: G-Set}
\label{sec:example-gset}

\TODO[parameterized over a set $\mathcal{X}$]

\begin{figure}[H]
  \centering
  \[
    \textsf{G-Set}_s(\mathcal{X}) = \left\{\begin{aligned}
      S &: \mathcal{P}(\mathcal{X}) \\
      s^0 &: \emptyset \\
      q &: \lambda x. x \in s \\
      u &: \lambda x. s \cup \{ x \} \\
      m &: \lambda s_1, s_2. s_1 \cup s_2 \\
    \end{aligned}\right.
  \]
  \caption{state-based \textsf{G-Set} \CRDT}
\end{figure}

\begin{figure}[H]
  \centering
  \[
    \textsf{G-Set}_o(\mathcal{X}) = \left\{\begin{aligned}
      S &: \mathcal{P}(\mathcal{X}) \\
      s^0 &: \emptyset \\
      q &: \lambda x. x \in s \\
      t &: \lambda x. (\textsf{ins}, x) \\
      u &: \lambda p. s \cup (\textsf{snd}~p) \\
      m &: \lambda s_1, s_2. s_1 \cup s_2 \\
    \end{aligned}\right.
  \]
  \caption{op-based \textsf{G-Set} \CRDT}
\end{figure}

\begin{figure}[H]
  \centering
  \[
    \textsf{G-Set}_\delta(\mathcal{X}) = \left\{\begin{aligned}
      S &: \mathcal{P}(\mathcal{X}) \\
      s^0 &: \emptyset \\
      q &: \lambda x. x \in s \\
      u &: \lambda x. \{ x \} \\
      m &: \lambda s_1, s_2. s_1 \cup s_2 \\
    \end{aligned}\right.
  \]
  \caption{$\delta$-state based \textsf{G-Set} \CRDT}
\end{figure}

\section{Consistency Guarantees}
\subsection{Eventual Consistency}
\subsection{Strong Eventual Consistency}
