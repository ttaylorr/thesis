\chapter{Introduction}

Computational systems today are larger than ever. Whereas previously one would
architect their programs to be run on a single system, it is now commonplace to
design programs that share computation across multiple machines which
communicate with each other in a coordinated fashion. It is natural, therefore,
to ask why one might design from the latter perspective rather than the former.
The answer is threefold:
\begin{enumerate}
  \item Resiliency. Designing a computational workload to be distributed amongst
    participants tolerates the failure of any one (or more) of those
    participants.
  \item Scalability. When designed from a distributed standpoint, ``scaling''
    your workload to meet a higher demand is reduced to adding additional
    hardware, not designing more efficient ways to do the computation.
  \item Locality. By sharing a computation across many individual pieces of
    hardware, system designers are able to place hardware closer to the site at
    which their requests originate. If the network between a user and some
    system is comparatively slow relative to the internal communication between
    hardware servicing that request, then hardware can be distributed closer to
    the user to reduce the overall latency in responding to a request.
\end{enumerate}

So, it is clear that as our demand on such computations grows, that it so too
must our need to design these systems in a way that considers resiliency,
scalability, and locality first.

In order to design systems in this way, however, one must consider additionally
the challenges imposed by not having access to shared memory amongst
participants in the computation. If a program runs in a single-threaded fashion
on a single computer, there is no need to coordinate memory accesses, since only
one part of the program may be reading or writing memory at a given time. If the
program is written to be multi-threaded, then the threads must coordinate
amongst themselves by using mutexes or communication channels to avoid
\textit{race conditions}.

The same challenge exists wben a system is distributed at the hardware and
machine level, rather than amongst multiple threads running on a single piece of
hardware. The challenge, however, is made more difficult by the fact that the
communication overhead is far higher between separate pieces of hardware than
between two threads.

\section{Preliminaries}

Our discussion here focuses on a particular class of datatypes that are designed
to be both easily distributed and require relatively low coordination overhead
by allowing individual participants to diverge temporarily from the state of the
overall computation\footnote{That is, the computation reflects a different
value depending on which participant in the computation responds to the
request}.

These datatypes operate in such a way so as to both avoid conflict between
concurrent updates, and to avoid locking and coordination overhead. These
datatypes are referred to as conflict-free replicated datatypes, hereafter,
CRDTs~\citep{shapiro11}. CRDTs are said to achieve \textit{strong eventual
consistency} (SEC) which is to say that they achieve a stronger form of
\textit{eventual consistency} (EC). We summarize the definitions of eventual-
and strong-eventual consistency from~\cite{shapiro11}.

\begin{definition}[Eventual Consistency]
  A replicated datatype is \textit{eventually consistent} if:
  \begin{itemize}
    \item Updates delivered to it are eventually delivered to all other replicas
      in the system.
    \item All well-behaved replicas that have received the same set of updates
      eventually reflect the same state.
    \item All executions on this datatype are terminating.
  \end{itemize}
\end{definition}

\begin{definition}[Strong Eventual Consistency]
  A replicated datatype is \textit{strong eventually consistent} if:
  \begin{itemize}
    \item It is eventually consistent, as above.
    \item Convergence occurs immediately, that is, any two replicas that have
      received the same set of updates \textit{always} reflect the same state.
  \end{itemize}
\end{definition}

\section{Overview}
Broadly speaking, there are two classes of CRDTs, which we refer to as the op-
and state-based variants. We will provide formal definitions for each of the two
classes in due time (particularly, we do so in Chapter~\ref{chap:background}),
but for now it is sufficient to distinguish the two as sending different kinds
of messages\footnote{It is sufficient to consider the op-based CRDT as sending
representations of actions: increment some value, place an item in a set, and so
on. On the other hand, it suffices to consider state-based CRDTs as sending the
representation of the effect of that action: my counter is now $x$, my set
includes these elements, and so on. state-based CRDTs are furthermore equipped
with a $\sqcup$ which is capable of merging two states together, and is
discussed in detail in Section~\ref{sec:background-state-based-crdts}.}.

These two classes are each well-suited for a specific kind of application.
Because the state-based CRDT needs to send a representation of its entire state,
it often requires a significant amount of network bandwidth to propagate large
messages \TODO[cite]. In return, networks supporting state-based CRDTs are
allowed to drop messages, and these state-based CRDTs still achieve SEC. On the
other hand, op-based CDRTs require relatively little network bandwidth to send
a notification of a single update, but in exchange demand that the network
deliver messages in-order and at-most-once \TODO[cite].

Significant work in this area~\cite{almedia18, enes18, cabrita17, vanDerLinde16}
has focused on mediating these two extremes. This line of research (particularly
in~\citep{almedia18}) has identified $\delta$-CRDTs---a variant of the
state-based CRDT which we discuss in
Section~\ref{sec:background-state-based-crdts}---as an alternative which
occupies a satisfying position between the two extremes.  $\delta$-CRDTs have,
in general, small payload size (thus requiring a similar amount of bandwidth as
messages sent and received from op-based CRDTs), but tolerate the same network
deficiencies as state-based CRDTs. This combination of properties makes them an
appealing alternative to traditional state- and op-based CRDTs, and places
interest in studying their convergence properties.

Our contribution builds on the work in~\citep{gomes17} and introduces a
set of formally verified proofs in Isabelle~\citep{wenzel02} that re-establish
the main result of~\citep{almedia18}, which we re-state here:

\begin{theorem}[Almedia, Shoker, Baquero, '18]
  Consider a set of replicas of a $\delta$-CRDT object, replica $i$ evolving
  along a sequence of states $X_i^0 = \bot$, $X_i^1=\ldots$, , each replica
  performing delta-mutations of the form $m^\delta_{i,k}(X^k_i)$ at some subset
  of its sequence of states, and evolving by joining the current state either
  with self-generated deltas or with delta-groups received from others. If each
  delta-mutation $m^\delta _{i,k}(X^k_i)$ produced at each replica is joined
  (directly or as part of a delta-group) at least once with every other replica,
  all replica states become equal.
\end{theorem}
